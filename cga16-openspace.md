# Introduction

Since the beginning of time, humans have looked up to the night sky in an attempt to understand the world around them. The movements of the Moon and stars and planets have shaped civilizations by producing calendars and aiding in navigation, while the shape of star patterns in the night sky served as seeds for many folk tales all around the world. Undoubtedly, the history of mankind has been shaped by the five planets, the Moon, and the 10000 or so stars that are visible to the naked eye. The invention of telescopes and satellites, however, has shown us that this is only a tiny slice of the eight planets, hundreds of moons, billions of galaxies, and countless stars and exoplanets in the entire observable universe.

Understanding the scientific discoveries and their results are key to understanding our cosmic origins, potential dangers, and the long-term future of our species. While small telescopes are readily available and allow amateur astronomers a deeper view into the cosmos, only a small enthusiastic group of people deal with the calibration, assembly, and usage of home telescopes. The rest of humanity has to rely on professional telescopes to produce stunning pictures of the night's sky that are the result of very long exposures which cannot be reproduced by the human eye. Unfortunately, the context in which these images are taken is lost in many cases, even when it comes to simple tasks as locating the photographed area on the sky. For many years, planetariums have provided this context to the public by reproducing the night sky to support explanations and tell a compelling story to the audience. These explanations are often provided by an expert in the field and provide easy access to the knowledge about the cosmos and all its wonders.

These guided experiences are useful and interesting events for the hundred or so visitors that can be seated in a planetarium, but they cannot reach a broader audience due to the physical limitations of a planetarium. With modern technology and the digitalization that all planetariums underwent during the last decade [1], it is now possible to enhance the presentation methods by utilizing immersive 3D computer graphics. This allows planetarium visitors to not only look up at the night's sky, but leave Earth virtually and watch any kind of content, which, in turn, enables the public dissemination of a much broader array of discoveries. As good as these developments have been, they have not solved the issue of scalability. One partial solution that has been employed is the ability to connect multiple planetariums for a shared one-to-many experience. In these cases, a single expert is presenting the information not only to a single planetarium, but their voice and video is transmitted to other connected planetariums in order to reach a broader audience. These solution, however, limit the distribution of lectures and explanations to bigger planetariums due to high costs that inhibit smaller venues to purchase the same specialized software to participate in these events.

By providing an astronomical software for free as open-source, it is possible to reach a much broader audience for these educational events. In addition, by designing the software in such a way that it can produce full-fidelity renderings on both planetarium surfaces as well as regular desktop or laptop monitors, the outreach can be exponentially bigger as a much larger audience can experience and participate from their home computers. While this removes the immersive experience of the explanations, it opens up the public access by an order of magnitude or more. We foresee, however, an upcoming change for the lack of immersion. The ongoing proliferation of headmounted displays, such as the Oculus Rift or the HTC Vive, make it possible to experience the same immersive experience on the home computer as is felt in a planetarium setting. And with the prices of these devices falling in the upcoming years, many more people will be able to afford and use such devices.

The second evolutionary step is the gradual transition in explanations from **what** has been discovered by telescopes and measurements over to **how** it is being discovered. The idea behind this is to not only show scientific results, but also explain the intricate process of the data acquisition itself. By making the general public part of the scientific discovery, it allows them to understand the underlying complexity and get a deeper appreciation for the collected data. This will ultimately result in a better educated populous that is more willing to spend the effort and resources to widen our collective scientific horizon.

The rest of this article is organized as follows: First, we will describe the basic conceptual ideas behind the open-source software *OpenSpace*, which can be used for these public disseminations before focussing on individual aspects of the software; the available data structures and data sources that and the domecasting ability to connect multiple software instances. Then, we visit the concept of science dissemination in greater detail with five focus topics, the *New Horizons*, *Rosetta*, and *Osiris Rex* space missions, prediction and research of space weather, and a feature called *Globe Browsing* to inspect high resolution images of planetary surfaces.

# Software
In order to facilitate the mass dissemination of astrophysical and astronomical data, we developed an extensible open-source software called *OpenSpace* that can be used as a platform to display a large variety of data sources. The system is not limited to any specific scientific domain, but has been developed to support a large number of contextualized visualizations, such as models, volumetric data, data-derived geometry, or abstract visualizations. By providing a rich modular framework for individual developers, it is possible to integrate new visualizations and keep the same look-and-feel and contextualized information that other people have contributed to the software as a whole, thus building an ecosystem of modules that can be used by anyone.

Many systems have previously provided such a service, such as Uniview, Digital Sky, Digistar, or Eyes on the Solar System, to name a few. Many of these systems are either vendor-specific or commercial products, which naturally limits their potential reach. In addition to requiring specialized hardware or licenses, thus putting a severe limitation on smaller venues that cannot generate enough revenue to allow for such an expense, the closed-source approach of these products also limits the variety of data that can be displayed. In these systems, content creators are limited to the rendering capabilities that are included and exposed in those softwares and are, usually, unable to write and integrate their own rendering modules that produce novel visualization techniques. While this tradeoff allows for a more stable codebase and end-user experience, it also limits the integration of new features, especially for venues that cannot afford to commission a custom rendering module written by the vendor directly.

A different approach to this tradeoff that we have taken is the use of open-source software, thus reducing the necessary investment to develop and include new rendering techniques by exposing the entire software for customization. One important historical software that have done this previously is Celestia, whose open-source development effectively stopped in 2010. With *OpenSpace* we are trying to build on the success of open-source software to provide a content-rich, community-curated environment for the general public as well as planetariums. Having a central instance that combines the efforts of small-to-medium venues and private citizens makes it possible to combine the benefits of a wide user base with the necessary expert knowledge for data integration. In the following section, we will elaborate on the building blocks that are used to generate these modules.

# Data Sources
The best visualization is useless if there is no data to visualize. Furthermore, it is absolutely necessary to display the available information scientifically accurate as to not mislead target audience of the visualization. In this section, we are describing some of the techniques and data sources.

## Dynamic Scene Graph
Integrating many large-scale astronomical data sets into a common reference frame can be taxing as the floating point representation of numbers in computers does not have infinite precision [2]. This restricts the relative scales of objects which can be represented in a single scene. The most widely used floating point number formats in computing are defined in the IEEE 754 standard as single and double precision with 32 bit and 64 bit respectively. Combining the floating decimal point and the finite storage means that the precision to represent numbers decreases with increasing magnitudes. This does not become an issue as long as objects of interest of similar magnitude are displayed, as a linear scaling factor can map value ranges into a more favorable distribution around 0. However, if objects of vastly different magnitudes are involved, this uniform scaling is no longer possible. As an example, the minimal value which can be added to 0 for single precision numbers is ~10^-7 while the minimal number that can be added to 10^10 is 1000. Every number smaller than this will be rounded to the same result. This also leads to the paradoxical situation that `while (f + 1 != f)` does not result in an infinite loop, but does always terminate.

Without any special consideration, this inhibits the use of a single coordinate system for objects even in our solar system. If the Sun is the origin of the coordinate system, the 32 bit floating point precision error at Pluto is so great, that the whole planet only contains 8 distinct distance values, with all intermediate values rounded to these distinct values (see Figure XX).

One possible solution to this issue is the concept of a ScaleGraph. We refer the reader to a paper by Klashed et al. for details on this [3]. In essence, the scene is described a tree that contains different scales as nodes. Each scale describes distances to child nodes and objects in its natural scaling. This graph structure then enables to translate and scale each visible object in the scene to be represented at the correct relative size without a loss of precision. However, due to the reprojection, issues regarding to stereoscopic rendering and scale transitions are introducted.

In *OpenSpace* we instead employ a Dynamic Scene Graph which is similar to the ScaleGraph, but does not utilize native scales and does not reproject objects to the current scale. Instead, all objects in the scene are rendered relative to the camera at their correct distances. This makes use of the fact for far away objects to be visible, they have to be sufficiently large, thus not making it necessary to render small variations on their surface (`bluah`).

## SPICE
In order to render astronomical objects in their correct positions, potentially complex trajectories have to be computed. In the easiest case of undisturbed orbits, the only information necessary are the six Keplerian elements from which the location of a body relative to a target reference frame can be retrieved. However, this simple representation does not capture any complex orbital mechanics such as the influence of additional bodies or effects from general relativity such as seen in, for example, the precession of Mercury. Therefore, a more sophisticated method for determining complex orbits has to be employed.

One widely used library called *SPICE* is provided by NASA and provides mission planning quality access to location and attitude of spacecraft, planets, and other objects through the use of *kernels*. One type of SPICE kernels provides location information for a specific time period. While these can be provided analytically, through the use of Keplerian elements, in most cases a higher order approximation or explicit sampling is used. These approximations or sampled locations are necessary for spacecraft with complex trajectories. Using the SPICE library provides instant access to all of this information and the capability to query locations in a variety of coordinate systems, allowing the use of the optimal coordinate system for each object. The SPICE library also provides many more features which are useful for visualization frameworks. For instance, it provides information about instruments onboard spacecrafts, such as the viewing angle, the shape of the field-of-view, or conversions regarding the on-board mission clock.

SPICE kernels are heavily used in the planning phase of various spacecraft missions, which has the additional benefit of making these kernels readily available for a large number of missions and also guarantee the accuracy of these kernels. While the scientific accuracy during the visualization has to be sufficiently high, the spacecraft planning and development phase necessitates an even higher level of accuracy.

## Digital Universe
> Reference A Flight through the Universe CISE

While most bodies in the solar system can be covered by using SPICE kernels, it is unfeasible to provide SPICE kernels for every star in the milky way and for each galaxy in the universe. Measurements of stars and galaxies have been performed by many different groups over many years, leading to many inconsistencies both in the collected data as well as the data formats.

One successful effort to curate all the available datasets is the Digital Universe, as published by the American Museum of Natural History. It collates all of the information, such as position, color, imagery, spectral measurements, and much more about visible stars in the milky way, as well as information about visible galaxies, quasars, and other measurements. By incorporating this database, it is possible to render a visually appealing and accurate representation of the night's sky to provide necessary context to activities in the solar system, let alone the possibility to integrate visualizations of events outside of the solar system.

The rendering of the stars is a particular aspect that is very error-prone. The stars are rendered as geometry shader generated camera-aligned billboards where the billboard is resized both according to the distance to the camera as well as the absolute brightness of the star; the closer and brighter a star is, the bigger the billboard becomes. This represents an artistic choice. In the idealized rendering, each star, regardless of distance and brightness would have the same size, which would be very close to a perfect point light source (Sirius, the brightest star, has an angular extent of 8.5 x 10^{-07} degrees from Earth). However, due to optical effects either in our eyes or telescopes, this point light source is subject to a point spread function creating an extended image. The result is depends on the apparent magnitude of the star, which, in turn, is dependent on the distance to the camera and the absolute brightness of the star. We simulate the point spread function of a perfect telescope on the billboard by a gaussian falloff. This results in a realistic and appealing visual representation (see Figure XX). By computing the apparent magnitude on-the-fly, we also account for the changing location of the camera when moving outside of the solar system. This allows all stars to be rendered in a way in which a telescope at the position of the virtual camera would perceive them. In addition, by exchanging the texture which is used for the point spread function, it is possible to simulate the visual appearance from different telescopes.

The same billboarded approach can be applied to all galaxies except the milky way. Instead of using a gaussian point spread function, the correct image as captured by telescopes is used instead with the billboard resized to reflect their accurate projected size regarding their location and distance to the camera. These images can either be reconstructed from optical light or any other detection method.

While inside our solar system, the Milky Way cannot be projected as a billboard as it surrounds camera in every direction. Hence we make use of a textured sphere that surrounds the solar system, which has an omnidirectional texture of the Milky Way mapped to its inside surface. For the texture we use a modified version of an all-sky survey by Mellinger [3]. A post-processing is applied to this image to remove all of the stars that are visible such that only the nebula `check with Carter on this` parts and the galactic dust of the galaxy remain. While this representation looks correct while the camera is in the solar system, the sphere has to be replaced by a simulated, volumetric representation of the Milky Way when the camera is outside the local area of the solar system and into intergalactic space. This volumetric data is generated from statistical models of star distribution in our solar system, which has been used to simulate the presence of various kinds of dust, the location of stars and much more. `Double check with Jon Parker, how about rotation?`

## Models & Planets
Apart from supporting the rendering of models using a large variety of standardized file formats, such as OBJ waveform, planetary surfaces are an important aspect impacting the visual fidelity of the system. For many planets in the solar system, such as Earth, Mercury, and Mars, humanity has acquired images of such high detail that simple straightforward texturing is unfeasible. Thus, a hierarchical level-of-detail approach is necessary that takes the distance of the camera to the planet, as well as the footprint of the planet on the display, into account. The same aspect applies to detailed terrain models, such as heightmaps, which are subsequently used to position other objects on the surface of a planet.

High resolution planetary images and heightmaps are usually distributed in a quad-tree level-of-detail approach. One of the most widely used protocols are the standardized Web Map Services (WMS), for which NASA's Global Image Browsing System (GIBS) is an example. These are web services where clients can request images for a specific region, which are delivered as images in a standardized format. It is up to the client software to then correctly align and display these images and interpolate between different detail levels. An additional requirement is to be able to layer different maps to highlight interesting features. For instance on Earth, there are maps of high resolution images of the surface, which have been collected over a large amount of time such that covering clouds can be removed, revealing the ground underneath for all locations. This map can be augmented with an additional layer showing only the current clouds without the background. Using this, it is possible to achieve a high resolution surface image with up-to-date cloud imagery, too.

Supporting a wide variety of standards for image loading and localization is an important part for specialization as many venues have access to higher detailed arial photography of their surroundings and need the ability to integrate this into the scene to provide a higher fidelity experience for visitors. Venues can make use of these techniques to integrate high resolution aerial photography of their local neighborhood to provide interesting tailor-made content. A database of these higher terrain resolutions could be a possible citizen scientist project enabled by our framework. Another example for this is the use of Mars Reconnaissance Orbiter's HiRise images of Mars, which have a sub-meter per pixel resolution, but are limited to a select few locations. Integrating this, and deriving higher resolution heightmaps from these images, is a worthwhile citizen science goal.

## Image Projection
Looking at the fleet of space craft in the solar system, there are many available modalities for the available instruments, such as radiometer, dust counter, magnetometer, ... . However, one of the instruments that is most intuitive for layman are image-generating cameras. From the cameras onboard the Luna 3 craft taking a picture of the far side of the moon in 1960, up to the modern cameras onboard the New Horizons space craft, images have always been inspiring the public and were hugely successful in generating public interest and understanding.

It is due to that reason that we focussed on the display of images first. The goal was to not only present the final image without context to the public, but also show the context of the acquisition time and the spatial relationship to the target body and other images taken. In order to achieve this, we make use of an image projection method presented by Everitt et al. [5] to project the images taken onto the target body. Using the camera's field-of-view, acquisition time, and pointing direction, it is possible to treat the camera as a virtual projector which casts the image onto the target body (see Figure~XX). This method automatically takes care of projection artifacts and is easily generalized to both multiple images as well as non-square image geometries. Furthermore, by including an additional virtual image plane which can be projected upon, it is possible to handle images in which the target object is filling the entire frame, or the subject of interest is on a physical body represented in the scene all together. This technique can be used to, for instance, capture the solar wind streaming out from the Sun, or the outgassing of material from comets (see Figure XX).

Additional complications present themselves when generalizing the image projection onto concave objects. While it is trivially applicable for spherical bodies as there is no self-shadowing on those bodies, it becomes more difficult on complex geometries. The standard image projection algorithm does not include any occlusion checks, which can lead to undesirable visual results (see Figure XX). The reason is that each location on the object is checked against the projected image independently, not considering other objects that may lie between the current location and the projection source. The solution to this problem lies in applying a raycasting type approach. By constructing a ray between each potential target location and the projection source, this ray can be checked against the geometry of the object to detect intersections and thus shadowing. If there is an intersection, then a previous point would be the correct point to project onto instead. While raytracing is usually a prohibitively expensive operation in real-time rendering, in this case it is admissible as it only has to be performed once for each image that is projected. With this method it is possible to project images not only onto regular bodies such as planets and moons, but also irregular bodies such as comets.

# Dome Casting
One important feature of *OpenSpace* that enables the mass public dissemination of scientific topics is the concept of **Domecasting**. Using a set of features, this allows multiple instances of *OpenSpace* to be cross-linked and replicate the same renderings on a multitude of distributed systems. The *master* node is in control of all connected *slave* nodes and every interaction that happens on the master node is replicated to the slaves, keeping them synchronized. Each instance of *OpenSpace* is running independently, but rendering settings (camera position, simulation time, rendering visual settings, etc) are synchronized over a network connection from the master to the client. Any small delay in his communication is acceptable as long as it stays well below a second. For a truly collaborative experience, it is also desirable to be able to change the which instance is designated as the *master*. This enables a ping-ponging between presenters that are physically separated and enables common decision making or information dissemination to a public.

In order to make the system more scalable, the network messages are not sent from the master to the slaves directly, but are routed through a server, which in turn distributes the messages to the slaves. On the one hand, this allows the creation of a priority list in the case that the outgoing network bandwidth is reached, making it possible to specify a set of $\alpha$ sites which will always be served first, with a set of $\beta$ and $\gamma$ sites in descending priority. On the other hand, it is the basis for load balancing, by introducing intermediate servers which simultaneously act as slave nodes to the main server and master to their own list of slaves. Furthermore, in many cases, the master node might be located in an area with severely limited outgoing bandwidth, such that the capacity would be quickly exceeded if multiple slaves were to connect. By placing the main server in a stable environment with a high-speed internet connection, we also allow the controlling of domecasting experiences from remote locations.

Each *OpenSpace* instance runs independently, instead of receiving a prerendered image, in order to support the local rendering geometry. For example, some slaves might run a multi-pipe planetarium setup while others run a single machine flat screen. By abstracting this away from any remote operations, the system becomes more stable and agnostic to rendering changes.

As mentioned before, the most important settings to be synchronized are the virtual camera information, the simulation time, as well as the change in simulation time (in simulation seconds per realtime second). Additional information are all settings which influence some aspect of the rendering, such as textures, adjustable transparencies, and others. Additionally, recorded audio and video needs to be transferred to all slaves which can then be displayed locally.

# Disseminations
Any kind of publicly funded scientific discovery has to be disseminated back into the public. In the case of astronomical discoveries, expensive and labor-intensive technology has to be utilized to achieve new discoveries, which naturally incurs a high cost. In order to justify these expenses, and garner continued future interest in these discoveries, the public has to know that they are participating and benefitting from these discoveries, either directly or vicariously. In the case of planetary missions, it is the increase in general knowledge of humanity that is desirable, as well as the discoveries that provide information about the creation of the solar system in addition to the more tangible effects of technological discovery.

Humans have an innate interest for the vastness of space and are eager to learn about places most people will never be able to physically see with their own eyes. Thus it is important to make these locations feel as real as possible to foster and nurture that innate curiosity.

One of the most successful methods of teaching, that is as old as humanity itself, comes in the form of a teacher-learner relationship. All the way from the antique until the beginning of the 20th century, this relationship has stayed fundamentally the same, there is one teacher interacting with learners which are all located in the same physical space. This tradition started to be broken by the use of radio shows and television, which provided the teachers with access to a large number of learners, which was not physically possible before. This, for the first time, also enabled the general audience to participate as learners, which previously was not possible as learners had to physically fit in the learning space and travel there.

Today, the use of e-learning material and free-for-all study courses provided on the internet has surged this form of learning. Any person with an internet connection can achieve a higher learning goal today than the wealthiest person just 100 years ago.

By creating a suitable software that supports this distributed learning by providing the context of the information that is transmitted, we can support learners throughout the world in understanding new concepts and ideas. For the best results, this knowledge transfer requires the presence of a teacher that is an expert in the field.

In the rest of this section, we will highlight four different parts, that we have been focussed on; 1. the *New Horizons* space craft performed a fly-by event at Pluto in July 2015 during which much information about this previously unvisited dwarf planet was collected; 2. The *Rosetta* spacecraft entered the orbit of the comet 67P/Churyumov-Gerasimenko in August 2014 and took measurements of the comet during its approach to the Sun, which increased the comets activity; 3. The study of the solar system conditions collectively known as *Space weather* In most cases, this refers to the conditions of the plasma being ejected from the Sun both as a continuous stream, the solar wind, as well as abrupt eruptions, such as coronal mass ejections; 4. a system for interactive display of high resolution level-of-detail planetary images. All these events had different parameters and objectives and provided a variety of challenges, whose solutions can now be applied to similar missions, such as Juno.

## New Horizons
NASA's mission flew by the Pluto system on July 14th, 2015 and took measurements with its seven instruments. Of special interest are the LORRI and RALPH instruments, that provided images of Pluto's its moons' surfaces, as well as REX that took radio measurements of Pluto's atmosphere. In our system, the images are projected using the method described above and the REX occultation measurements are represented by a line connecting the space craft and Earth. The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping, for example the SWAP instrument measures solar wind density values. This mission was disseminated to the larger public during a public, global event in which 13 different locations throughout the world participated. During a 2h live show, which coincided with New Horizons closest approach to Pluto, experts on the mission team explained details of the desired outcome, using OpenSpace as the source of the contextualization for these informations. In addition to the live audience in the participating locations, a video stream of the event was available on the Internet, which was also later provided as a video-on-demand, called "Breakfast at Pluto".

## Rosetta
After the arrival of ESA's mission at the comet 67P/Churyumov-Gerasimenko, it has since been orbiting the comet at various altitudes, providing measurements about comets activity, its mass, and the amount of outgassing. Among the instruments that we utilized sofar are the fairly low resolution NAVCAM, which is primarily used for navigational purposes, but its images were released to the public much earlier than from other instruments. The OSIRIS camera is a high-resolution 2048$\times$2048 pixel optical camera with a narrow angle and wide angle lens. Images from both instruments can be used with the same image projection technique as described above, utilizing the addition of the image projection technique for concave objects.  

## Osiris Rex

## Space Weather
- Bringing the scientist tools (iSWA) into planetariums
- NASA's Living with a Star program

## Globe Browsing

[1] Sumners, Carolyn, and Patricia Reiff. "Creating Full-Dome Experiences in the New Digital Planetarium." In NASA Office of Space Science Education and Public Outreach Conference, vol. 319, p. 155. 2004.
[2] Goldberg, David. "What every computer scientist should know about floating-point arithmetic." ACM Computing Surveys (CSUR) 23, no. 1 (1991): 5-48.
[3] Klashed, Staffan, Per Hemingsson, Carter Emmart, Matthew Cooper, and Anders Ynnerman. "Uniview-Visualizing the Universe." Eurographics 2010-Areas Papers (2010).
[4] Mellinger, Axel. "A color all-sky panorama image of the Milky Way." Publications of the Astronomical Society of the Pacific 121, no. 885 (2009): 1180.
[5] Everitt, Cass, Ashu Rege, and Cem Cebenoyan. "Hardware shadow mapping." White paper, nVIDIA 2 (2001).