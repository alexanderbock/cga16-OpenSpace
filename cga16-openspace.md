Title: OpenSpace -- Changing the narrative of public disseminations from **what** to **how**
Authors: Alexander Bock, Emil Axelsson, Carter Emmart, Masha Kuznetsova, Anders Ynnerman

# Introduction

Since the dawn of time, humans have looked up to the night sky in an attempt to understand the world around them.  The movements of the Moon and stars and planets have shaped civilizations by producing calendars and aiding in navigation, while the shape of star patterns in the night sky inspired folk tales all around the world.  Undoubtedly, the history of mankind has been shaped by our Moon, the five planets, and the 10000 or so stars that are visible to the naked eye. Telescopes and satellites, however, show that this is only a tiny slice of the eight planets, hundreds of moons, billions of galaxies, and countless stars and exoplanets in the observable universe.

Understanding scientific discoveries and their results are key to understanding our cosmic origins, potential dangers, and our long-term future. While small telescopes are readily available and allow amateur astronomers a deeper view into the cosmos, only a small enthusiastic group deal with the calibration, assembly, and usage of home telescopes.  The rest of humanity has to rely on professional telescopes to produce stunning pictures of the night's sky that are the result of very long exposures which cannot be reproduced by the human eye.  Unfortunately, the context in which these images are taken is lost in many cases, even when it comes to simple tasks as locating the photographed area on the sky.  For many years, planetariums have provided this context to the public by reproducing the night sky to support explanations and tell a compelling story to the audience.  These explanations are often provided by an expert in the field and grant easy access to the knowledge about the cosmos and all its wonders.

These guided experiences are useful and interesting events for the hundred or so visitors that can be seated in a planetarium, but they cannot reach a broader audience due to the physical limitations of a planetarium.  With modern technology and the digitalization that all planetariums underwent during the last decade [1], it is now possible to enhance the presentation methods by utilizing immersive 3D computer graphics.  This allows planetarium visitors to not only look up at the night's sky, but leave Earth virtually and watch any kind of content, which, in turn, enables the public dissemination of a much broader array of discoveries.  As important as these developments have been, they have not solved the issue of scalability.  One partial solution is the ability to connect multiple planetariums for a shared one-to-many experience.  In these cases, a single expert is presenting the information not only to a single planetarium, but their voice and video is transmitted to other connected planetariums in order to reach a broader audience.  These solution, however, limit the distribution of lectures and explanations to bigger planetariums due to high costs that inhibit smaller venues to purchase the same specialized software to participate in these events.

By providing an astronomical software for free as open-source that can perform these tasks, it is possible to reach a much broader audience for these educational events.  In addition, by designing the software in such a way that it can produce full-fidelity renderings on both planetarium surfaces as well as regular desktop or laptop monitors, the outreach can be exponentially bigger as a much larger audience can experience and participate these disseminations from their home computers.  While this removes the immersive experience of the explanations, it opens up the public access by an order of magnitude or more.  We foresee, however, an upcoming change for the lack of immersion. The ongoing proliferation of headmounted displays, such as the Oculus Rift or the HTC Vive, make it possible to experience a similar immersive experience on the home computer as is felt in a planetarium setting. With the prices of these devices falling in the upcoming years, many more people will be able to afford and use such devices.

The second evolutionary step is the gradual transition in explanations from **what** has been discovered by telescopes and measurements over to **how** it is being discovered.  The idea behind this is to not only show scientific results, but also explain the intricate process of the data acquisition itself. By making the general public part of the scientific discovery, it allows them to understand the underlying complexity and get a deeper appreciation for the collected data.  This will ultimately result in a better educated populous that is more willing to spend the effort and resources to widen our collective scientific horizon.

The rest of this article is organized as follows: First, we will describe the basic conceptual ideas behind the open-source software *OpenSpace*, which provides the capabilitise for these public disseminations before focussing on individual aspects of the software; the available data structures and data sources that and the domecasting ability to connect multiple software instances.  Then, we visit the concept of science dissemination in greater detail with five focus topics, the *New Horizons*, *Rosetta*, and *Osiris Rex* space missions, prediction and research of space weather, and a feature called *Globe Browsing* to inspect high resolution images of planetary surfaces.

# Software
In order to facilitate the mass dissemination of astrophysical and astronomical data, we developed an extensible open-source software called *OpenSpace* that can be used as a platform to display a large variety of data sources.  The system is not limited to any specific scientific domain, but has been developed to support a large number of contextualized visualizations, such as geometric models, volumetric data, data-derived geometry, or abstract visualizations.  By providing a rich modular framework for developers, it is possible to integrate new visualizations and keep the same look-and-feel and contextualized information that other people have contributed to the software as a whole, thus building an ecosystem of modules that can be used by anyone.

We believe that the benefits of interactive presentations outweigh the higher production value of prerendered content.  By directly or indirectly interacting with the data sources, the learning effect is increased [2].  This is especially true for concurrent activities that benefit from a quick turnaround time for explanations.

Many systems have previously provided such a service, such as Uniview, Digital Sky, Digistar, or Eyes on the Solar System, to name a few.  Many of these systems are commercial products generally limiting their potential reach.  In addition to requiring specialized hardware or licenses, thus putting a severe limitation on smaller venues that cannot generate enough revenue to allow for such an expense, the closed-source approach of these products also limits the variety of data that can be displayed.  In these systems, content creators are limited to the rendering capabilities that are included and exposed in those softwares and are, usually, unable to write and integrate their own rendering modules that produce novel visualization techniques.  This tradeoff limits the integration of new features, especially for venues that cannot afford to commission a custom rendering module written by the vendor directly.

We have taken a different approach to this tradeoff that through the use of open-source software, thus reducing the necessary investment to develop and include new rendering techniques by exposing the entire software for customization.  One important historical software that have done this previously is Celestia, whose open-source development effectively stopped in 2010.  With *OpenSpace* we are building on the success of open-source software to provide a content-rich, community-curated environment for the general public as well as planetariums.  Having a central instance that combines the efforts of small-to-medium venues and private citizens makes it possible to combine the benefits of a wide user base with the necessary expert knowledge for data integration.  In the following section, we will elaborate on the building blocks that are used to generate these modules.

# Data Sources
The best visualization is useless if there is no data to visualize.  Furthermore, it is absolutely necessary to display the available information scientifically accurate as to not mislead target audience of the visualization.  In this section, we are describing some of the techniques and data sources.

## Dynamic Scene Graph
Integrating many large-scale astronomical data sets into a common reference frame can be taxing as the floating point representation of numbers in computers does not have infinite precision [3].  This restricts the relative scales of objects which can be represented in a single scene.  The most widely used floating point number formats in computing are defined in the IEEE 754 standard as single and double precision with 32 bit and 64 bit respectively.  Combining the floating decimal point and the finite storage means that the precision to represent numbers decreases with increasing magnitudes.  This does not become an issue as long as objects of interest of similar magnitude are displayed.  However, if objects of vastly different magnitudes are involved, an accurate representation is no longer possible.  As an example, the minimal value that can be added to a single precision floating point value v is ~10^7 * v.   Every number smaller than this will be rounded to the same number.  This also leads to the paradoxical situation that `while (f + 1 != f)` does not result in an infinite loop, but does always terminate.  Without any special consideration, this inhibits the use of a single coordinate system for objects even in our solar system.  If the Sun is the origin of the coordinate system, the 32 bit floating point precision error at Pluto results in the whole planet only containing 8 distinct distance values, with all intermediate values clamped to these distinct values.

One possible solution to this issue is the concept of a ScaleGraph. We refer the reader to a paper by Klashed et al. for details on this [4].  In essence, the scene is described a tree that contains different scales as nodes.  Each scale describes distances to child nodes and objects in its natural scaling.  This graph structure then enables to translate and scale each visible object in the scene to be represented at the correct relative size without a loss of precision.  However, due to the reprojection, issues regarding to stereoscopic rendering and scale transitions are introducted.

In *OpenSpace* we instead employ a Dynamic Scene Graph which is similar to the ScaleGraph, but does not utilize native scales and does not reproject objects to the current scale [5].  Instead, all objects in the scene are rendered relative to the camera at their correct distances.  This makes use of the fact for far away objects to be visible, they have to be sufficiently large, thus not making it necessary to render small variations on their surface.

## SPICE
In order to render astronomical objects in their correct positions, potentially complex trajectories have to be computed.  In the easiest case of undisturbed orbits, the only information necessary are the six Keplerian elements from which the location of a body relative to a target reference frame can be retrieved.  However, this simple representation does not capture any complex orbital mechanics such as the influence of additional bodies or effects from general relativity such as seen in, for example, the precession of Mercury.  Therefore, a more sophisticated method for determining complex orbits has to be employed.

One widely used library called *SPICE* is provided by NASA and provides mission planning quality access to location and attitude of spacecraft, planets, and other objects through the use of *kernels*.  One type of SPICE kernels provides location information for a specific time period.  While these can be provided analytically, through the use of Keplerian elements, in most cases a higher order approximation or explicit sampling is used.  These approximations or sampled locations are necessary for spacecraft with complex trajectories.  Using the SPICE library provides instant access to all of this information and the capability to query locations in a variety of coordinate systems, allowing the use of the optimal coordinate system for each object.  The SPICE library also provides many more features which are useful for visualization frameworks.  For instance, it provides information about instruments onboard spacecrafts, such as the viewing angle, the shape of the field-of-view, or conversions regarding the on-board mission clock.

SPICE kernels are heavily used in the planning phase of various spacecraft missions, which has the additional benefit of making these kernels readily available for a large number of missions and also guarantee the accuracy of these kernels.  While the scientific accuracy during the visualization has to be sufficiently high, the spacecraft planning and development phase necessitates an even higher level of accuracy.

## Digital Universe
> Reference A Flight through the Universe [X]

![Background Context](images/context.png)
Figure 1. An accurate and visually pleasing representation of the night's sky provides adequate context to mission visualization.

Measurements of stars and galaxies have been performed by different research groups over many years, leading to many inconsistencies in the provided data formats and coverages.  One successful effort to curate all available datasets is the *Digital Universe*, published by the American Museum of Natural History.  It collates information, such as position, color, images, and spectral measurements, about visible stars in the Milky Way, as well as data about galaxies, quasars, and other celestial objects.  By incorporating this extensive database, it is possible to render a visually appealing and accurate representation of the night's sky (see Figure 1) to provide necessary context to activities in the solar system, let alone the possibility to integrate visualizations of events outside of the solar system and provide a three dimensional view on the stars closest to the Sun.

The rendering of the stars is a particular aspect that is very error-prone.  Stars are rendered as camera-aligned billboards where the billboard is resized according to the distance to the camera as well as the absolute brightness of the star; the closer and brighter a star is, the bigger the billboard.  This represents an artistic choice.  In the idealized rendering, each star, regardless of distance and brightness would have the same size, which would be very close to a perfect point light source (Sirius, the brightest star, has an angular extent of 8.5 x 10^{-07} degrees from Earth).  However, our eyes or telescopes apply a point spread function that extends this point into an image.  The result depends on the apparent magnitude of the star, which, in turn, depends on the distance to the camera and the absolute brightness of the star.  We simulate the point spread function of a perfect telescope on the billboard by a Gaussian falloff.  This results in a realistic and appealing visual representation.  By interactively computing the apparent magnitude, we also account for the changing location of the camera when moving outside of the solar system.  This produces a rendering of stars as a telescope at the position of the virtual camera would perceive them.  In addition, by exchanging the texture which is used for the point spread function, it is possible to simulate the visual appearance from different telescopes.

For all galaxies except the Milky Way, a similar billboard technique can be applied.  However, instead of aligning the billboard to the camera, it has to be aligned to the location of the Sun in order to provide the ability to fly close to a distant galaxy and retain the view as seen from Earth.  Instead of using a Gaussian point spread function, the image as captured by various telescopes is used with the billboard resized to reflect their accurate projected size.  These images can either be reconstructed from optical light or any other detection method. 

![Image](images/galaxy.png)
Figure 2. A volumetric rendering of the Milky Way using a statistical distribution of stars and gas.

While being inside our solar system, the Milky Way cannot be projected as a billboard as it surrounds the camera in every direction.  Hence we make use of a textured sphere that has an omnidirectional texture of the Milky Way mapped to its inside surface as an environment map.  For the texture we use a modified version of an all-sky survey by Mellinger [6] in which a post-processing has been applied to remove all of visible stars.  While this representation looks correct if the camera is inside our solar system, the sphere has to be replaced by a simulated, volumetric representation of the Milky Way when the camera is outside the local area of the solar system.  This volumetric data is generated from statistical models of star distribution in our solar system, which has been used to simulate the presence of various kinds of dust, the location of stars and much more (see Figure 2).

## Planetary Rendering
Apart from supporting the rendering of models using a large variety of standardized file formats, planetary surfaces are an important aspect for the visual fidelity of the system.  For many planets in the solar system, such as Earth, Mercury, or Mars, humanity has acquired images of such high detail that simple texturing is unfeasible and a hierarchical level-of-detail approach is necessary which takes the distance of the camera to the planet, as well as the planet's visual footprint into account.  The same aspect applies to detailed terrain models, such as heightmaps, which are subsequently used to position other objects on the surface of a planet.

![Image](images/himalaya.png)
Figure 3. Showing the mountains of the Himalayas using the ESRI World Imagery dataset.

![Image](images/mars.png)
Figure 4. Showing the surface of Mars as reconstructed from the Mars Reconnaissance Orbiter's Context camera. [TODO: Remove timestamp]

An important aspect in visualizing scientific discoveries is providing an adequate context.  Similar to using the stars and the Milky Way as a background for mission and space weather visualization, it is necessary to provide a rich context for the Earth sciences.  To this end, we make use of a widely used set of standards for the delivery of planetary images called *Web Map Service* [7].  These standards perscribe a level-of-detail technique to request image tiles on a planet and allow an application to resolve very high resolution details.  By treating local height information as a greyscale image, it furthermore becomes possible to produce terrain renderings with the same processes (see Figure 3).  One use case for this application is the fleet of Earth-orbiting spacecraft that gather a variety of measurements, for example the ozone layer, pollutants, or the temperature.  Another use case is a high fidelity rendering of other planetary surfaces.  For Mars, there exist a large database of surface images retrieved from the Mars Reconnaissance Orbiter that can be used to create realistic surface renderings of planets that most people will likely never see in person (see Figure 4).

Supporting a wide variety of standards for image loading is an important aspect for specialization as many venues have access to high-resolution arial photography of their surroundings and want to integrate this to provide a higher fidelity experience for visitors.  Creating a database of these higher terrain resolutions is a potential citizen scientist project enabled by our system.

## Image Projection
Distributed in the solar system, the human fleet of space craft has a large variety of available instruments, such as radiometer, dust counters, or magnetometers, to gather information.  However, one of the instruments that is most intuitive for the general public are image-generating cameras.  From the cameras onboard Luna 3 taking grainy pictures of the far side of the moon in 1960, up to the modern high-resolution cameras onboard the New Horizons space craft, images have always inspired the public and are hugely successful in generating public interest and understanding.

![Image](images/image-proj.jpg)
Figure 5. Showing the footprint of the LORRI camera on Pluto' surface after taking three pictures. [TODO: Redo without info text]

![Image](images/image-proj2.png)
Figure 6. Projecting onto an additional image plane enables the visualization of outgassing, in this case on the comet 67P/Churyumov–Gerasimenko. [TODO: Redo without info text]

Our main goal is not to only present the final images, but show the context of their acquisition and the spatial relationship to the target body and other images. In order to achieve this, we make use of an image projection method presented by Everitt et al. [7] to project the images onto the target body.  Using information about the camera's field-of-view, acquisition time, and pointing direction, it is possible to treat the camera as a virtual projector which casts the image onto the target body (see Figure 5).  This method takes resolves projection artifacts and is easily generalized to include multiple images or non-square image geometries.  Furthermore, by including an additional virtual image plane to project upon, it is possible to handle images in which the target object is not filling the entire frame or the subject of interest is not a physical body (see Figure 6).

While this method is easily applicable for spherical bodies as there is no self-shadowing on those bodies, it becomes more difficult on complex, concave geometries.  The standard image projection algorithm does not include any occlusion checks, which can lead to undesirable visual results.  One possible solution for this uses a shadow map algorithm that identifies the target object's first projection point for given camera frustum.  Only the points that are not in shadow from the camera are projected on.  With this method it is possible to project images not only onto regular bodies such as planets and moons, but also irregular bodies such as the comet 67P/Churyumov–Gerasimenko (see Figure 6).

# Dome Casting
![Image](images/collab.jpg)
Figure 7. Utilizing the dome casting feature enables multiple experts to collaboratively explain difficult topics to a public audience.

One important feature of *OpenSpace* enabling the mass public dissemination of scientific topics is the concept of **Domecasting**. Using a set of features that allow multiple instances of *OpenSpace* to be cross-linked and to replicate renderings on a multitude of distributed systems, it becomes possible to share explanations across the globe (see Figure 7).  All interactions are replicated to all connected nodes, keeping them synchronized.  Each instance of *OpenSpace* is running independently, but rendering settings are synchronized over a network connection.  These rendering settings are information about the virtual camera, the simulation time, changes to the simulation time, as well as other information influencing some aspect of the rendering, such as textures, adjustable transparencies, or enabling and disabling objects.  Any sufficiently small delay in his communication is acceptable.  For a truly collaborative experience with presenters that are physically separated, it is also desirable to pass control between different application instances, thus enabling common decision making or information dissemination to a public.  Each *OpenSpace* instance runs independently, instead of receiving a prerendered image, in order to support the local rendering geometry.  For example, some slaves might run a multi-pipeline planetarium setup while others run a single machine flat screen.  By abstracting this away from any remote operations, the system becomes more stable and agnostic to rendering changes.  In order to support venues that do not have the computational resources to perform a rendering locally, the software has the capabilities to stream a 360 degree rendering, which can be locally projected instead.

# Disseminations
Any kind of publicly funded scientific discovery has to be disseminated back into the public. In the case of astronomical discoveries, expensive and labor-intensive technology has to be utilized to achieve new discoveries. In order to justify these expenses, and garner continued future interest, the public has to participate and benefit from these discoveries directly or vicariously. In the case of planetary missions, it is the increase in general knowledge of humanity that is desirable, as well as the discoveries that provide information about the creation of the solar system in addition to the more tangible effects of technological advances. Humans have an innate interest for the vastness of space and are eager to learn about places most people will never be able to physically see with their own eyes. Thus it is important to make these locations feel as real as possible to foster and nurture that innate curiosity.

One of the most successful methods of teaching, that is as old as humanity itself, is the teacher-learner relationship. From the antique until the beginning of the 20th century, this relationship has stayed fundamentally the same as there is one teacher interacting with learners located in the same physical space. This tradition broken with the use of radio shows and television, provideding the teachers with access to a large number of learners. This, for the first time, also enabled the general audience to participate as interested learners, which previously was not possible. Today, the use of e-learning material and MOOC material provided on the internet has surged this form of learning. Any person with an internet connection can achieve a higher learning goal today than the wealthiest person just 100 years ago. By creating a suitable software that supports this distributed learning by providing the context of the information that is transmitted, we can support learners throughout the world in understanding new concepts and ideas. For the best results, this knowledge transfer requires the presence of a teacher that is an expert in the field.

In the rest of this section, we highlight four different dissemination topics that we have, so far, worked on; 1. the *New Horizons* space craft performed a fly-by event at Pluto in July 2015, collecting much information about this previously unvisited dwarf planet; 2. the *Rosetta* spacecraft entered the orbit of the comet 67P/Churyumov-Gerasimenko in August 2014, performed measurements of the comet during its approach to the Sun, which increased the comets activity, and ended its mission in September 2016; 3. the study of the solar system conditions collectively known as *Space weather*. In most cases, this refers to the conditions of the plasma being ejected from the Sun both as a continuous stream, the solar wind, as well as abrupt eruptions, such as coronal mass ejections; 4. a system for interactive display of high resolution level-of-detail planetary images, exemplified in the rendering of Earth and Mars. All these events had different parameters and objectives and highlight a variety of challenges, whose solutions can now be applied to similar events.

## New Horizons
![Image](images/newhorizons.png)
Figure 8. New Horizons' view of Pluto and its moons. The active LORRI camera's frustum is shown extending from the space craft.

NASA's mission flew by the Pluto system on July 14th, 2015 and took measurements with its seven instruments. Of special interest are the LORRI and RALPH instruments, that provided images of Pluto, its moons' surfaces, as well as REX, measuring Pluto's atmosphere. In our system, the images are projected using the method described above and the REX occultation measurements are represented by a line connecting the space craft and Earth (see Figure 8). The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping. This mission was disseminated to about 2500 people during a public, global event in which 13 different locations participated. During a 2h live show, which coincided with New Horizons closest approach to Pluto, experts on the mission team explained details of the desired outcome using *OpenSpace* as the source of the contextualization for these informations. In addition to the live audience in the participating locations, a video stream of the event was available on the Internet, which was also later provided as a video-on-demand, called "Breakfast at Pluto".

## Rosetta
![Image](images/rosetta.jpg)
Figure 9. Projecting images on the concave comet 67P/Chryumov-Gerasimenko.

ESA's Rosetta mission orbited the comet 67P/Churyumov-Gerasimenko between August 2014 and September 2016 at various altitudes, providing measurements about comets activity, its mass, and the amount of outgassing. Among the instruments that we utilized are the fairly low resolution NAVCAM, which is primarily used for navigational purposes, but its images were released to the public much earlier than from other instruments. The OSIRIS camera is a high-resolution `2048 x 2048` pixel optical camera with a narrow angle and wide angle lens. Images from both instruments can be used with the same image projection technique as described above, utilizing the addition of the image projection technique for concave objects (see Figure 9). An interesting aspect of this mission is the complexity of the space craft's trajectory around the comet and thus the ability to explain complicated aspects of orbital mechanics with Rosetta as an example. These range from the ability to measure the weight of the comet based on the deflection of the orbit, the avoidance of outgassing aphelion, or the release of the lander Philae.

## Osiris Rex
Osiris Rex is a NASA mission that will orbit the asteroid Bennu and perform a sample return mission. It launched in September 2016 and will end with a landing in 2023. During its stay at the asteroid, Osiris Rex will take detailed images of the entire surface of the asteroid. This information is then used to select a sample site from which the asteroid samples will be collected. In an event at the American Museum of Natural History, one of mission scientists explained the entire plan of the mission from launch to touch down to the public audience. This mission is especially interesting as the space craft has to change its geometry to be able to collect samples from the asteriod without touching it or contaminating the samples.

## Space Weather
Space Weather research is concerned with plasma conditions in the solar system ultimately caused by activity on the Sun. These conditions have a great impact on both satellites as well as life on Earth thus making predictions very important. Combining this with the complicated topic warrants the use of visualization techniques to bridge the gap between expert knowledge and public understanding. Therefore, we integrated the scientists Integrated Space Weather Analysis toolkit, developed at the Community Coordinated Modeling Center into *OpenSpace* such that the results can be directly disseminated to the public using the same expert analysis tools. This also ties directly into NASA's Living with a Star program, whose mission is to improve the understanding of the Sun and how it affects the entire solar system. One part of this work that deals with improving the forecasting capabilities by producing comparisons between simulations and in situ measurements was presented previously [8].

# Conclusion
In this paper, we presented the beginning stages of an open-source framework aiding the public dissemination of **how** scientific discoveries are made rather than focussing on **what** has been discovered. By making the software freely available, it is possible to reach a much broader audience than was possible before by limiting the explanations to a planetarium venue. This also creates opportunities for citizen science projects that curate the vast amount of information that we have gathered about the universe that surrounds us.

[1] Carolyn Sumners and Patricia Reiff. "Creating Full-Dome Experiences in the New Digital Planetarium." In NASA Office of Space Science Education and Public Outreach Conference, vol. 319, p. 155. 2004.
[2] Bertrand Schneider, Patrick Jermann, Guillaume Zufferey, and Pierre Dillenbourg. "Benefits of a Tangible Interface for Collaborative Learning and Interaction." IEEE Transactions on Learning Technologies, Vol. 4, No. 3, (2011)
[3] David Goldberg. "What every computer scientist should know about floating-point arithmetic." ACM Computing Surveys 23, no. 1 (1991).
[4] Staffan Klashed, Per Hemingsson, Carter Emmart, Matthew Cooper, and Anders Ynnerman. "Uniview-Visualizing the Universe." Eurographics 2010-Areas Papers (2010).
[5] Emil Axelsson, Jonathas Campi Costa, Claudio Silva, Carter Emmart, Alexander Bock, Anders Ynnerman, "Enabling Scaling, Positioning, and Navigation in the Universe". EuroVis (2017)
[6] Axel Mellinger. "A color all-sky panorama image of the Milky Way." Publications of the Astronomical Society of the Pacific 121, no. 885 (2009): 1180.
[7] Cass Everitt, Ashu Rege, and Cem Cebenoyan. "Hardware shadow mapping." White paper, nVIDIA 2 (2001).
[8] Alexander Bock, Asher Pembroke, M. Leila Mays, Lutz Rastaetter, Timo Ropinski, and Anders Ynnerman. "Visual verification of space weather ensemble simulations." In 2015 IEEE Scientific Visualization Conference (2015).
[9] Open Geospatial Consortium. "OpenGIS Web Map Service version 1.3.0" (2006).

[X] Miguel A. Aragon-Calvo, and Mark SubbaRao. "A Flight through the Universe." Computing in Science & Engineering 17, no. 6 (2015).