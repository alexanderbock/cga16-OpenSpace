Title: OpenSpace -- Changing the narrative of public dissemination from **what** to **how**

Authors: Alexander Bock, Emil Axelsson, Carter Emmart, Masha Kuznetsova, Anders Ynnerman

# Introduction

Since the dawn of time humans have looked into the night sky in an attempt to understand the mysteries of the world.  The movements of the Moon, the stars and the planets have guided the rise of civilizations by producing calendars and aiding in navigation, while the shape of patterns in the night sky has inspired tales told all around the world.  The history of humankind has been shaped by our Moon, the five planets and the roughly one thousand stars that are visible to the naked eye.  In more recent times, since the invention of telescopy, ground based large scale telescopes and satellite based instruments have enabled us to see that what we humans can see is no more than a tiny slice of an observable universe consisting of billions of galaxies, innumerable stars and, most recently, exoplanets which have only been identified in the last twenty-five years.

Understanding the process of scientific discoveries and their results is key to understanding our cosmic origins, potential dangers, and our long-term future.  While small telescopes are readily available, and allow amateur astronomers a somewhat deeper view into the cosmos, only a small and enthusiastic group of people are sufficiently dedicated to deal with the complexity of the calibration, assembly, and usage of home telescopes.  The rest of humanity has to rely on professionally operated telescopes to collect the data and produce the stunning pictures of the night's sky that are the result of very long exposures which cannot be seen with the human eye.  Unfortunately, the context within which these images are taken is often lost, even when it comes to the most simple of tasks such as locating the photographed area in the sky.  For many years, planetariums have tried to provide this context to the public by reproducing the night sky on a dome surface and support explanations and try to tell compelling stories.  These explanations are often provided by an expert and grant easy access to the knowledge about the cosmos and all its wonders.

In this article we describe a software platform named *OpenSpace* which paves the path for the next generation of science communication and data exploration in the astrophysical and space exploration domain.  It capitalizes on the digitalization of planetariums undergone during the last decade [1], state-of-the art immersive 3D computer graphics [2], the development of novel visualization methods, and access to unique data sources.  It is, furthermore, an open source platform designed to support community efforts, both in the provision of new software features and new content.

Even though OpenSpace targets high-end installations, such as planetariums and dome theatres, it supports a wide range of platforms and thus has a much wider reach than proprietary planetarium software and can even be used on home computers for free.  While home use currently removes the immersive experience it opens up the public access enormously.  Furthermore, the ongoing proliferation of headmounted displays, such as the Oculus Rift or the HTC Vive, will make it possible for a home user to have a similarly immersive experience using their own home computer as can be had in the planetarium setting.  With the prices of these devices continually falling, many more people will be able to afford and utilize such devices.

At the core of the narratives that OpenSpace enables lies an evolutionary step in science communication that scientists and educators promoting and supporting.  This is the gradual transition in explanations from **what** has been discovered by telescopes and measurements over to **how** it is being discovered.  The idea behind this is to not only show scientific results, but to also explain the planning stages and the intricate processes of the data acquisition itself.  By including the general public in the process of scientific discovery it allows the audience to understand the underlying complexity and gain a deeper appreciation for the data collected.  This will ultimately result in a better educated populous who are more willing to spend the effort and resources needed to widen our collective scientific horizons.

The dissemination of scientific information and the processes by which it is reached is a very complicated problem.  Providing that information in a context which enables the public to understand both aspects of the nature of scientific information is extremely important, and the narratives used can be vital in informing the public and enabling them to understand the importance of such work and its influence upon their lives and their future.  In the world of education the idea of 'doing beats telling' is a very common refrain and the purpose of the tools described in this paper is to provide for both.  The platform enables joint scientific exploration by expert groups, dissemination by the knowledgeable audiences of all sizes, as well as independent learning by students looking for insight.  The interactive nature of the exploration provided in the OpenSpace environment permits far greater learning benefits than the use of pre-rendered informational content which, while providing solid and reliable information, does not permit the audience either the pleasure or the impact of personal exploration.  The open source nature of the platform will also provide an opportunity for smaller venues, whose revenues may not be sufficient to support purchase of software licences or professional software development, to offer such an exploratory approach to scientific knowledge dissemination.

In this paper we present the open source platform *OpenSpace*, describe its basic concepts for data exploration and dissemination, and demonstrate its application through four specific applications: the *New Horizons* space craft fly-by event at Pluto in July 2015, the *Rosetta* space craft entered the orbit of the comet 67P/Churyumov-Gerasimenko in August 2014, performing measurements during its approach to the Sun, the study of the solar system conditions collectively known as *Space weather*, and a system for interactive display of high resolution level-of-detail planetary images, exemplified in the rendering of Earth and Mars.  All these events had different parameters and objectives and highlight a variety of challenges, whose solutions are applicable to similar events.
[MDC comment: Shortened version above. Ditch the rest. You don't really need the short version either but a tease can be fun. (AY comment: If we need more space this whole section can go)
The rest of this article is organized as follows: First, we will describe the basic concepts behind the open-source software *OpenSpace*, which provides the capabilities needed for public dissemination before focussing on individual aspects of the software exemplified by four applications;  1. the *New Horizons* space craft fly-by event at Pluto in July 2015, collecting much information about this previously unvisited dwarf planet;  2. the *Rosetta* space craft entered the orbit of the comet 67P/Churyumov-Gerasimenko in August 2014, performing measurements during its approach to the Sun, and ending its mission in September 2016 by landing on the comet;  3. the study of the solar system conditions collectively known as *Space weather*, which refers to the conditions of the plasma being ejected from the Sun both as a continuous stream, the solar wind, as well as abrupt eruptions, such as coronal mass ejections;  4. a system for interactive display of high resolution level-of-detail planetary images, exemplified in the rendering of Earth and Mars.  All these events had different parameters and objectives and highlight a variety of challenges, whose solutions are applicable to similar events.]

# The OpenSpace software platform

(AY comment:  We should list the demand we have on the software to support the ambitions described above: 1) Support a variety of data sources including astrophysical data respositories, simulation data, and object geometries 2) Support dynamic and time varying data 3) Handle the large dynamic range of coordinates i space 3) Support large scale display environments such as dome heaters[MDC: cause they do get cold!] 4) Support collaborative and distributed visualization 5) Support interaction paradigms tailored to different use cases and environments 6) Be generally available through open source licensing.)

To meet the demands specified above we have developed an extensible open-source software platform named *OpenSpace*.  The system is not limited to any specific scientific domain but has been designed to support a large number of contextualized objects, such as geometric models, volumetric data, data-derived geometry, or abstract data derived from a wide variety of sources including astrophysical data respositories or simulation data.  An important aspect of these datasets are dynamic and time-varying processes which have to be accurately captured.  The software is being developed in close collaboration with the user community, astonomers, space physics researchers, and the planetarium community.  By providing a rich, modular framework that is open source for developers, they are able to extend and integrate new visualization approaches and build an ecosystem of modules that is accessible and can be used by anyone.  Furthermore, then open source license enables the software to support a large variety of platforms, such as large scale display environments or immersive theaters.  This, in turn, enables the collaborative and distributed visualization that benefits the wider public.

[AB: the paragraph above is new and should be checked]

(AY comment: the following two paragraphs have to be rewritten by Alex. They should describe the software architecture and make the point that is is based on a state-the-art computer graphics and visualization tools. The comparison with other software and benefits of OpenSource should be toned down. Rule: don’t talk about what others can’t do, talk about what you can do)

[MDC: I put a few lines in the final paragraph of the introduction which are derived from this. Some has been included below. Suggest ditch the rest. (AY comment: parts of this can be used in the introduction?) We believe that the benefits of interactive presentation outweigh the higher production value of prerendered content. This is especially true for concurrent activities that benefit from a rapid turnaround time for explanations.  Other applications have previously provided such a service, such as Uniview, Digital Sky, Digistar, Eyes on the Solar System, and others.  Many of these systems are commercial products which is limiting their potential reach.  In addition to requiring specialized hardware or licenses, putting a severe limitation on smaller public visitor venues that cannot generate enough revenue to allow for such an expense, the closed-source approach of these products also limits the variety of data that can be displayed.  In these systems, content creators are limited to the rendering capabilities that are included and exposed in those softwares and are, usually, unable to write and integrate their own software and data modules that introduce novel visualization techniques and access to unique data. This tradeoff limits the integration of new features, especially for venues that cannot afford to commission a custom software module written by the vendor directly.]

Other applications have previously provided a similar service, such as Uniview, Digital Sky, Digistar, Eyes on the Solar System, and others.  Many of these systems are commercial products and may require specialized hardware or software licences which limits their potential outreach to venues which have sufficient revenue to afford them.  In addition, the closed-source nature of these products limits the variety of data that can be displayed.  Through the use of an open-source approach we have reduced the necessary investment for the community to develop and include new rendering techniques, incorporate new data types and sources and so produce an environment which can benefit a whole new generation of information providers and students.  One important historical application that has attempted to address this problem in the domain of space science is Celestia, whose open-source development effectively stopped in 2010.  With *OpenSpace* we are building on the success of open-source software to provide a content-rich, community-curated environment for the general public as well as planetariums.  Having a central instance that combines the efforts of small-to-medium venues and private citizens makes it possible to combine the benefits of a wide user base with the necessary expert knowledge for data integration.  In the following sections, we describe the building blocks that have been used to generate the current platform.

## Dynamic Scene Graph
Integrating many large-scale astronomical data sets into a common reference frame can be taxing as the floating point representation of numbers in computers does not have infinite precision [3].  This restricts the relative scales of objects which can be represented in a single scene.  The most widely used floating point number formats in computing are defined in the IEEE 754 standard as single and double precision with 32 bit and 64 bits, respectively.  Combining the floating decimal point and the finite storage means that the precision of the numerical representation of numbers decreases with increasing magnitudes.  This does not become an issue as long as objects of similar magnitude are displayed.  However, if objects on vastly different scales are involved, an accurate representation is no longer possible.  As an example, the minimum value that can be added to a single precision floating point value, $v$, is $\approx 6*10^8 \cdot v$.  Every number smaller than that will be reduced to zero change. Without any special consideration, this inhibits the use of a single coordinate system for objects even in our solar system.  In the case of Pluto and Sun being the origin of the coordinate system, the distance of $\approx ~5*10^9$ km results in a distance of about 300 km between representable floating point values.

One possible solution to this issue is the concept of a ScaleGraph [4] which is a modified scene graph that contains different scales as intermediate nodes.  Each scale describes distances to child nodes and objects in their canonical scaling. This graph structure then enables translation and scaling each visible object in the scene to be represented at the correct relative size without a loss of precision.  However, due to a necessary reprojection, issues regarding to stereoscopic rendering and scale transitions are introduced.  Another solution is the use of additional bytes to represent a greater range of numbers.  Fu and Hanson developed Power Scaled Coordinates which use a fourth floating point value to represent an expontial component [5].

In *OpenSpace* we instead employ a Dynamic Scene Graph which is similar to the ScaleGraph, but does not utilize native scales and does not explicitly reproject objects to the current scale [6].  Instead, all objects in the scene are rendered relative to the camera at their correct distances with the closest object to the camera always serving was the global coordinate system origin.  This makes use of the fact that for far away objects to be visible, they have to be sufficiently large, and can be rendered with a low lever of detail.

## SPICE
In order to render astronomical objects in their correct positions, potentially complex trajectories have to be computed.  In the easiest case of undisturbed orbits, the only information necessary are the six Keplerian elements from which the location of a body relative to a target reference frame can be retrieved.  However, this simple representation does not capture any complex orbital mechanics such as the influence of additional bodies or effects from general relativity such as seen in, for example, the precession of Mercury.  Therefore, a more sophisticated method for determining complex orbits has to be employed.

One widely used library named *SPICE* is provided by NASA and supports mission planning quality access to location and attitude of space craft, planets, and other objects through the use of *kernels*.  One type of SPICE kernels provides location information for a specific time period.  While these can be provided analytically, in most cases a higher order approximation or explicit sampling strategy is used for space craft with complex trajectories.  Using the SPICE library provides instant access to all of this information and the capability to query locations in a variety of coordinate systems, enabling use of an optimal coordinate system for each object.  Furthermore, it provides information about instruments onboard space crafts, such as the viewing angle, the shape of the field-of-view, or conversions regarding the on-board mission clock.

SPICE kernels are heavily used by space craft operators in the planning phase of various missions, which has the additional benefit of making these kernels readily available for a large number of missions and also guarantees the accuracy of the kernels, as the scientific accuracy during in visualization has to be sufficiently high, but the space craft planning and development phases necessitate an even higher level of accuracy.

## Digital Universe
![Background Context](images/context.png)
Figure 1. An accurate and visually well designed representation of the night's sky provides necessary spatial context to any solar system-based visualization.  This image shows 100000 stars from the Hipparcos and Gliese star catalogs.

Measurements of stars and galaxies have been performed over the cause of many years, leading to  inconsistencies in the provided data formats and coverages.  One successful effort to curate all available datasets is the *Digital Universe*, hosted by the American Museum of Natural History.  It collates information such as position, color, images, and spectral measurements of many visible stars in the Milky Way, as well as data about galaxies, quasars, and other celestial objects.  By incorporating this extensive database, it is possible to render a visually appealing and accurate 3D representation of the night's sky (see Figure 1) to provide necessary context to activities in the solar system, let alone the possibility to integrate visualizations of events outside of the solar system and provide a three dimensional view on the stars closest to the Sun.

Stars are rendered as camera-aligned billboards where the billboard is resized according to the distance to the camera as well as the absolute brightness of the star; the closer and brighter a star is, the bigger the billboard.  This represents an artistic choice.  In a more physically correct rendering, each star, regardless of distance and brightness would have the same size, which would be very close to a perfect point light source (Sirius, the brightest star, has an angular extent of $8.5 \cdot 10^{-7}$ degrees from Earth).  However, our eyes and telescopes apply a point spread function that extends this point into an image.  The result depends on the apparent magnitude of the star, which, in turn, depends on the distance to the camera and the absolute brightness of the star.  We simulate the point spread function of an idealized telescope on the billboard by applying a Gaussian falloff.  This results in a realistic and appealing visual representation.  By interactively computing the apparent magnitude, we also account for the changing location of the camera when moving outside of the solar system.  This produces a rendering of stars as a telescope at the position of the virtual camera would perceive them.  In addition, by exchanging the texture which is used for the point spread function, it is possible to simulate the visual appearance from different telescopes.

For all galaxies except the Milky Way, a similar billboard technique can be applied.  However, instead of aligning the billboard to the camera, it has to be aligned to the location of the Sun in order to provide the ability to fly close to a distant galaxy and retain the view as seen from Earth.  Instead of using a Gaussian point spread function, the image as captured by various telescopes is used while the billboard is resized to reflect their accurate projected size.  These images can either be reconstructed from optical light or any other detection method. 

![Image](images/galaxy.png)
Figure 2. A volumetric rendering of the Milky Way using a statistical distribution of stars and gas provides context to visualizations of the local neighborhood and our Sun's position inside the Milky Way.

While being inside our solar system, the Milky Way cannot be projected as a billboard as it surrounds the camera in every direction.  Hence we make use of a textured sphere that has an omnidirectional texture of the Milky Way mapped to its inside surface as an environment map.  For the texture we use a modified version of an all-sky survey by Mellinger [7] to which a post-processing has been applied to remove all  visible stars.  While this representation looks correct if the camera is inside our solar system, the sphere has to be replaced by a simulated, volumetric representation of the Milky Way when the camera is outside the local area of the solar system.  This volumetric data is generated from statistical models of matter distribution in our galaxy, which has been used to simulate the presence of various kinds of dust, the location of stars and much more (see Figure 2).

## Planetary Rendering
An important aspect in visualizing scientific discoveries is providing an adequate context such as, representations of planetary surfaces are an important aspect for the visual fidelity of the system.  For many planets in the solar system, such as Earth, Mercury, or Mars, humanity has acquired images of such high detail that simple texturing is unfeasible and a hierarchical level-of-detail approach is necessary which takes the distance of the camera to the planet, as well as the planet's visual footprint into account.  The same aspect applies to detailed terrain models, such as heightmaps, which are subsequently used to position other objects on the surface of a planet.

![Image](images/himalaya.png)
Figure 3. A large number of datasets are available for Earth.  This case uses the ESRI World Imagery dataset to show the mountains of the Himalayas with the Moon in the sky.  High-resolution images, digital terrain models, and annotations are available for large areas our planet.

![Image](images/mars.png)
Figure 4. High-resolution datasets are also available for other planets of our solar system, such as Mars.  The images and digital terrain models are constructed from the Mars Reconnaissance Orbiter's Context and HiRISE cameras, whose primary goal is to map future landing sites.

To this end, we make use of a widely used set of standards for the delivery of planetary images called *Web Map Service* [8].  These standards enable a level-of-detail technique to request image tiles on a planet and supports very high resolution details.  By treating local height information as a greyscale image, it furthermore becomes possible to produce 2.5D terrain rendering with the same methods (see Figure 3).  One use case for this application is data from the fleet of Earth-orbiting space craft gathering a variety of measurements, for example the ozone layer, pollutants, or temperatures.  Another use case is a high fidelity rendering of other planetary surfaces.  For Mars, there exist a large database of surface images retrieved from the Mars Reconnaissance Orbiter that can be used to create realistic surface renderings of planets that no human has observed in real-life (see Figure 4) [9].

Supporting a wide variety of standards for image loading is an important aspect for specialization as many public venues have access to high-resolution arial photography of their surroundings and want to integrate this to provide a higher fidelity experience for visitors to a specific site.  Creating a database of these higher terrain resolutions is a potential citizen scientist project enabled by our system.

## Image Projection
Distributed in the solar system, the human fleet of space craft has a large variety of available instruments, such as radiometer, dust counters, or magnetometers, to gather information.  However, one of the instruments that is most intuitive for the general public are image-generating cameras.  Ranging from the cameras onboard Luna 3 taking grainy pictures of the far side of the moon in 1960, to the modern high-resolution cameras onboard the New Horizons space craft taking pictures of Pluto, produced images have always inspired the public and are hugely successful in generating public interest and understanding.

![Image](images/image-proj1.png)
Figure 5. This image exemplifies the image acquisition of New Horizons' LORRI camera on Pluto's surface and shows the improved image quality of this image campaign over previous Hubble images.  The raw, uncorrected images provide the public with an insight into the complexity of mosaic image acquisition.

![Image](images/image-proj2.png)
Figure 6. By projecting images onto an additional image plane it becomes possible to visualize comet outgassing as it occurs, in this case, on the comet 67P/Churyumov-Gerasimenko as it approached the Sun and the resulting space craft trajectory considerations.

Our goal when visualizing space missions is not to only present the final images, but to show the context of their acquisition and the spatial relationship to the target body and other images.  In order to achieve this, we make use of an image projection method presented by Everitt et al. [10] to project the images onto the target body.  Using information about the camera's field-of-view, acquisition time, and pointing direction, it is possible to treat the camera as a virtual projector that casts the image onto the target body (see Figure 5).  This method resolves projection artifacts and is easily generalized to include multiple images or non-standard image geometries.  Furthermore, by including an additional virtual image plane to project upon, it is possible to handle images in which the target object is not filling the entire frame or the subject of interest is not a physical body (see Figure 6).

While this method is easily applicable to spherical bodies as there is no self-shadowing on those bodies, it becomes more difficult on complex, concave geometries.  The standard image projection algorithm does not include any occlusion checks, which can lead to undesirable visual results.  One possible solution for this uses a shadow map algorithm that identifies the target object's first projection point for a given camera frustum.  Only the points that are not in the shadow from the camera are used for the projection.  With this method it is possible to project images not only onto regular bodies such as planets and moons, but also irregular bodies such as the comet 67P/Churyumov–Gerasimenko (see Figure 6).

### New Horizons
![Image](images/newhorizons.png)
Figure 7. New Horizons' view of Pluto and its moons a few hours before closest encounter. The active LORRI camera's frustum is shown extending from the space craft and is used to contextualize the image acquisition on Pluto and its moons.  In addition, the extending shadow of Pluto provides the public with an intuition of the direction towards the inner solar system.

NASA's New Horizons mission flew by the Pluto system on July 14th, 2015 and made measurements with its seven instruments.  Of special interest are the LORRI and RALPH instruments, providing images of Pluto, Charon, and other moons' surfaces, as well as the REX instrument, measuring Pluto's atmosphere (see Figure 8).  In OpenSpace The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping. This mission was disseminated to about 2000 people during a public, global event in which 13 different locations participated.  During a 2h live show, which coincided with New Horizons closest approach to Pluto, experts on the mission team explained details of the desired outcome using *OpenSpace* to contextualize the operations of the instruments concurrent with the fly-by.  In addition to the live audience in the participating locations, a video stream of the event was available on the Internet, which was also later provided as a video-on-demand, called "Breakfast at Pluto".

### Rosetta
![Image](images/rosetta.jpg)
Figure 8. Projecting multiple images onto an accurate model of the concave comet 67P/Chryumov-Gerasimenko enables the explanation of space craft operations, taking, for example, the varying phase angles into account.

ESA's Rosetta mission orbited the comet 67P/Churyumov-Gerasimenko between August 2014 and September 2016 at various altitudes while providing measurements about comet's activity, its mass, and amount of outgassing.  Among the utilized instruments are the fairly low resolution NAVCAM, which is primarily used for navigational purposes, but its images were released to the public much earlier than from other instruments.  The OSIRIS camera is a high-resolution $2048 \cross 2048$ pixel optical camera with a narrow angle and wide angle lens.  Images from both instruments can be used with previously described image projection (see Figure 9).  An interesting aspect of this mission is the complexity of the space craft's trajectory around the comet and thus the ability to explain complicated aspects of orbital mechanics using Rosetta as an example.  These range from the ability to measure the weight of the comet based on the deflection of the orbit, the avoidance of outgassing at the comet's aphelion, or the release of the Philae lander.

### Osiris Rex
Osiris Rex is a NASA mission that will orbit the asteroid Bennu and perform a sample return mission.  It launched in September 2016 and will end with a landing back at Earth in 2023.  During its stay at the asteroid, Osiris Rex will take detailed images of the entire surface of the asteroid.  This information is then used to select a sample site from which the asteroid samples will be collected.  In an event at the American Museum of Natural History, one of the mission scientists explained the entire plan of the mission from launch to touch down to the public audience.  This mission is especially interesting as the space craft has to change its geometry to be able to collect samples from the asteriod without touching it or contaminating the acquired samples.

## Space Weather
Space Weather research is concerned with plasma conditions in the solar system ultimately caused by activity on the Sun.  These conditions have a great impact on both satellites as well as life on Earth thus making predictions very important.  Combining this with the complexity of the topic warrants the use of visualization techniques to bridge the gap between expert knowledge and public understanding.  Therefore, we integrated the scientists' Integrated Space Weather Analysis toolkit, developed at the Community Coordinated Modeling Center at NASA Goddard into *OpenSpace* such that the results can be directly disseminated to the public using the same expert analysis tools.  This also ties directly into NASA's Living with a Star program, whose mission is to improve the understanding of the Sun and how it affects the entire solar system.  One part of this work that deals with improving the forecasting capabilities by producing comparisons between simulations and in situ measurements was presented previously [11].

# Dome Casting
![Image](images/collab.jpg)
Figure 9. Utilizing the dome casting feature enables a collaborative session including multiple experts that explains complex topics to a large public audience.  By streaming these sessions online, their potential reach is improved by orders of magnitude.  This New Horizons presentation has been seen by over 50000 people worldwide.

One important feature of *OpenSpace* enabling the mass public dissemination of scientific topics is the concept of **Domecasting**.  Using a set of features that allow multiple instances of *OpenSpace* to be cross-linked and to replicate renderings on a multitude of distributed systems, it becomes possible to share explanations across the globe (see Figure 9).  Each instance of *OpenSpace* is running independently, but rendering settings are synchronized over a network connection.  These rendering settings are information about the virtual camera, the simulation time, changes to the simulation time, as well as other information influencing some aspect of the rendering, such as textures, adjustable transparencies, or enabling and disabling objects.  Any sufficiently small delay in his communication is acceptable as only information about changes is transmitted.  For a truly collaborative experience with presenters that are physically separated, it is also desirable to pass control between different application instances, thus enabling common decision making or information dissemination to a public.  Each *OpenSpace* instance runs independently, instead of receiving a prerendered image, in order to support the local rendering geometry.  For example, some clients might run a multi-pipeline planetarium setup while others run a single machine flat screen.  By abstracting this away from any remote operations, the system becomes more stable and agnostic to rendering changes.  In order to support venues that do not have the computational resources to perform a rendering locally, the software has the capabilities to stream a 360 degree rendering, which can be locally reprojected.

# Conclusion
In this paper, we presented an open-source framework called *OpenSpace* that supports the public dissemination of **how** scientific discoveries are made rather than purely focussing on **what** has been discovered.  This capabilities have been demonstrated in four different use cases, showing the usability of such an approach when performing public presentations.  By making the software freely available, it is possible to reach a much broader audience than previous solutions that limited explanations to larger planetarium venues.  This also creates opportunities for citizen science projects that curate the vast amount of information that we have gathered about the universe that surrounds us and thus ultimately educate humanity.

[1] Carolyn Sumners and Patricia Reiff. "Creating Full-Dome Experiences in the New Digital Planetarium." In NASA Office of Space Science Education and Public Outreach Conference, vol. 319, p. 155. 2004.
[2] Miguel A. Aragon-Calvo, and Mark SubbaRao. "A Flight through the Universe". Computing in Science & Engineering 17, no. 6 (2015).
[3] David Goldberg. "What every computer scientist should know about floating-point arithmetic". ACM Computing Surveys 23, no. 1 (1991).
[4] Staffan Klashed, Per Hemingsson, Carter Emmart, Matthew Cooper, and Anders Ynnerman. "Uniview-Visualizing the Universe". Eurographics 2010-Areas Papers (2010).
[5] Phillip Fu, Andrew Hanson. "A transparently scalable visualization architecture for exploring the universe". IEEE Transactions on Visualization and Computer Graphics (2011).
[6] Emil Axelsson, Jonathas Campi Costa, Claudio Silva, Carter Emmart, Alexander Bock, and Anders Ynnerman, "Enabling Scaling, Positioning, and Navigation in the Universe". EuroVis (2017)
[7] Axel Mellinger. "A color all-sky panorama image of the Milky Way". Publications of the Astronomical Society of the Pacific 121, no. 885 (2009): 1180.
[8] Open Geospatial Consortium. "OpenGIS Web Map Service version 1.3.0". (2006).
[9] Karl Bladin, Emil Axelsson, Erik Broberg, Carter Emmart, Patric Ljung, Alexander Bock, and Anders Ynnerman, "Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization". IEEE Transactions on Visualization and Computer Graphics (2017).
[10] Cass Everitt, Ashu Rege, and Cem Cebenoyan. "Hardware shadow mapping". White paper, nVIDIA 2 (2001).
[11] Alexander Bock, Asher Pembroke, M. Leila Mays, Lutz Rastaetter, Timo Ropinski, and Anders Ynnerman. "Visual verification of space weather ensemble simulations". In 2015 IEEE Scientific Visualization Conference (2015).

